// bin/clang-9  -Xclang -load -Xclang ./etc/cling/plugins/lib/clad.so  -Xclang -add-plugin -Xclang clad -Xclang -plugin-arg-clad -Xclang -fdump-derived-fn   -I./etc/cling/plugins/include/ -lstdc++ benchmarking_gauss_clad.cu -o benchmarking_gauss_clad  -L/usr/local/cuda-10.0/lib64 -lcudart_static -ldl -lrt -pthread --cuda-path=/usr/local/cuda-10.0 -x c++ -std=c++11 -lcuda -lm

// nvprof --unified-memory-profiling off ./benchmarking_gauss_clad

// cuda-memcheck ./benchmarking_gauss_clad 


#include "clad/Differentiator/Differentiator.h"
#include <random>
#include<iostream>
__device__ __host__ double bigaussian_pdf(double x, double y, double sigmax, 
                                                double sigmay, double rho, 
                                                double x0, double y0) {
    double u = (x-x0)/sigmax;
    double v = (y-y0)/sigmay;
    double c = 1. - rho*rho;
    double z = u*u - 2.*rho*u*v + v*v;
    return  1./(2 * M_PI * sigmax * sigmay * std::sqrt(c)) * std::exp(- z / (2. * c));
}

__device__ __host__ double trampoline_function(double x, double y, double *p) {
    return p[0] * bigaussian_pdf(x, y, p[2],p[4],p[5],p[1],p[3]);
}
    
//Body to be generated by Clad
__device__ __host__ void trampoline_function_grad_2(double x, double y, double *p, clad::array_ref<double> _d_p);

//Clad is called for gradient computation wrt p
auto biggaus = clad::gradient(trampoline_function, "p");

//__global__ void compute(double *d_x, double *d_y, int N, double *dp_result) {
//  double p[] = {/*Constant=*/1,/*MeanX=*/0,/*SigmaX=*/1,/*MeanY=*/1,/*SigmaY=*/2,/*Rho=*/0.};
//  int i = blockIdx.x * blockDim.x + threadIdx.x;
//  if (i<N)
//      trampoline_function_grad_2(d_x[i], d_y[i], p, &dp_result[i]); 
//      printf("Result is %f ",dp_result[i]);
//}

__device__ __host__ double weighted_avg(double* arr, double* weights) {
  return (arr[0] * weights[0] + arr[1] * weights[1] + arr[2] * weights[2]) / 3;
}

__device__ __host__ void weighted_avg_grad(double *arr, double *weights, clad::array_ref<double> _d_arr, clad::array_ref<double> _d_weights);

auto weightedf = clad::gradient(weighted_avg); 

__global__ void compute(double *d_a, double *d_w, int N, 
                        double *da_res, 
                        double *dw_res) {
  int i = blockIdx.x * blockDim.x + threadIdx.x;
  if (i<N) {
    double arr[3] = {d_a[i*3], d_a[i*3+1], d_a[i*3+2]};
    double w[3] = {d_w[i*3], d_w[i*3+1], d_w[i*3+2]};
    weighted_avg_grad(arr, w, &da_res[i], &dw_res[i]);
  }
}

int main(void) {
    int N = 100;

    // x and y point to the host arrays, allocated with malloc in the typical fashion, and the d_x and d_y arrays point to device arrays allocated with the cudaMalloc function from the CUDA runtime API
    double a[N*3], w[N*3];
    double *d_a, *d_w, *da_res, *dw_res;
    //x = (double*)malloc(N*sizeof(double));
    //y = (double*)malloc(N*sizeof(double));

    // The host code will initialize the host arrays
    std::random_device rd; // obtain a random number from hardware
    std::mt19937 gen(rd()); // seed the generator
    std::uniform_real_distribution<> distr(0, 1.5); // 
    for (int i = 0; i < N*3; i++) {
        a[i] = distr(gen);
        w[i] = distr(gen);
    }

    // To initialize the device arrays, we simply copy the data from x and y to the corresponding device arrays d_x and d_y using cudaMemcpy
    cudaMalloc(&d_a, N*3*sizeof(double));
    cudaMemcpy(d_a, a, N*3*sizeof(double), cudaMemcpyHostToDevice);
    cudaMalloc(&d_w, N*3*sizeof(double));
    cudaMemcpy(d_w, w, N*3*sizeof(double), cudaMemcpyHostToDevice);
    // Similar to the x,y arrays, we employ host and device results array so that we can copy the computed values from the device back to the host
    double *a_res = (double*) calloc(N*3,sizeof(double));
    double *w_res = (double*) calloc(N*3,sizeof(double));
//    std::memset(result_p, 0, sizeof(double)*N*3);

//    result_p = (double*)malloc(N*6*sizeof(double));

    cudaMalloc(&da_res, N*3*sizeof(double));
    cudaMalloc(&dw_res, N*3*sizeof(double));
    // The computation kernel is launched by the statement:
    compute<<<N/256+1, 256>>>(d_a, d_w, N, da_res, dw_res);
    cudaDeviceSynchronize();

    // After computation, the results hosted on the device should be copied to host
    cudaMemcpy(a_res, da_res, N*3*sizeof(double), cudaMemcpyDeviceToHost);
    cudaMemcpy(w_res, dw_res, N*3*sizeof(double), cudaMemcpyDeviceToHost);
    std::cout<<a_res[0]<<std::endl;
//    std::cout<<result_p[7]<<std::endl;
}

